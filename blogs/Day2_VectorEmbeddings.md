# ğŸ“Š Day 2: Vector Embeddings & Semantic Search - Making Documents Searchable

**Duration:** 2 hours | **Type:** Core Technical | **Difficulty:** Intermediate

---

## 1ï¸âƒ£ SESSION FLOW

### What We'll Cover Today (Step-by-Step)

1. **Embeddings Fundamentals** (15 min)
   - What are embeddings?
   - Text â†’ Vectors: the transformation
   - Why embeddings enable semantic search

2. **Introduction to FAISS** (15 min)
   - What is FAISS and why use it?
   - How it differs from keyword search
   - Real-world performance benefits

3. **Creating Your Vector Store** (20 min)
   - Hands-on: Converting chunks to embeddings
   - Building the FAISS index
   - Understanding vector dimensions

4. **Semantic Similarity Search** (20 min)
   - Live demo: Search by meaning, not keywords
   - Comparing results vs. keyword search
   - Understanding relevance scores

5. **Persistence & Loading** (15 min)
   - Saving your vector store
   - Loading pre-built indexes
   - Optimization strategies

6. **Architecture Review** (15 min)
   - How embeddings fit into RAG
   - Preview of Day 3 (LLM integration)
   - Q&A

7. **Hands-On Exercise** (20 min)
   - Students build their own vector store
   - Test with sample queries

---

## 2ï¸âƒ£ LEARNING OBJECTIVES

By the end of Day 2, students will be able to:

âœ… **Understand Embeddings**
- Explain what text embeddings are conceptually
- Describe how embeddings enable semantic search
- Compare embeddings vs. traditional keyword search
- Understand vector dimensions and similarity metrics

âœ… **Master FAISS Vector Store**
- Create FAISS indexes from document chunks
- Add/remove documents from vector stores
- Understand time/space complexity trade-offs
- Save and load persisted indexes

âœ… **Perform Semantic Search**
- Query vector stores semantically
- Retrieve top-K similar documents
- Interpret similarity scores
- Filter and re-rank results

âœ… **Optimize for Production**
- Choose appropriate embedding models
- Understand dimension reduction trade-offs
- Implement efficient batch operations
- Plan for scaling to millions of documents

**Prerequisites:** Day 1 knowledge, basic linear algebra (dot product, distance)

**Key Concepts:** Embeddings, vector spaces, FAISS, semantic similarity, cosine distance, top-K retrieval

---

## 3ï¸âƒ£ THEME: THE SEMANTIC TRANSLATOR

### Real-World Context

**Scenario:** You're a librarian with 100,000 books. Someone asks: *"Show me books about machine learning optimization."*

**The Traditional Approach (Keyword Search):**
- Search for "machine learning" AND "optimization"
- Get every book with those exact words
- Including: "A machine that learns to optimize toasters" âŒ
- Missing: "Gradient descent optimization techniques" (uses different words) âŒ

**The Modern Approach (Semantic Search):**
- Understand that "machine learning optimization" is about *algorithms improving themselves*
- Find books about: gradient descent, neural network training, hyperparameter tuning
- Automatically group related concepts
- Understand meaning, not just words âœ…

**Why This Day?**
- Day 1 prepared the documents (cut them into pieces)
- Day 2 teaches the documents to "think" in vectors (understand meaning)
- Day 3 uses vectors to answer questions
- Day 4 puts it all in an interface

---

## 4ï¸âƒ£ PRIMARY GOAL

### What You'll Build

A **Semantic Search Engine** that:

1. âœ… Converts text chunks to embeddings
2. âœ… Stores embeddings in FAISS
3. âœ… Performs similarity search by meaning
4. âœ… Returns relevant results with scores
5. âœ… Persists and loads vector stores efficiently

### Architecture: Day 2 in the RAG Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EMBEDDINGS & VECTOR STORE                â”‚
â”‚                                                            â”‚
â”‚  INPUT (from Day 1)                                       â”‚
â”‚    â†“                                                       â”‚
â”‚  Chunk 1: "Machine learning is..." â”€â”€â†’ [0.32, -0.18, ...] â”‚
â”‚  Chunk 2: "Deep learning models..." â”€â”€â†’ [0.31, -0.19, ...] â”‚
â”‚  Chunk 3: "Gradient descent..." â”€â”€â”€â”€â”€â†’ [0.30, -0.17, ...] â”‚
â”‚    â†“                                                       â”‚
â”‚  EMBEDDING MODEL (HuggingFace Transformer)                â”‚
â”‚  "sentence-transformers/all-MiniLM-L6-v2"                â”‚
â”‚  â”œâ”€ Input: Text                                           â”‚
â”‚  â”œâ”€ Output: 384-dimensional vector                        â”‚
â”‚  â””â”€ Speed: Fast (runs on CPU)                             â”‚
â”‚    â†“                                                       â”‚
â”‚  FAISS INDEX                                              â”‚
â”‚  â”œâ”€ Store vectors efficiently                             â”‚
â”‚  â”œâ”€ Enable fast similarity search                         â”‚
â”‚  â””â”€ Support millions of documents                         â”‚
â”‚    â†“                                                       â”‚
â”‚  QUERY: "What about optimization?"                        â”‚
â”‚  â”œâ”€ Convert to embedding: [0.32, -0.17, ...]             â”‚
â”‚  â””â”€ Find similar: Chunk 3 âœ“, Chunk 1 âœ“, Chunk 2 âœ“        â”‚
â”‚                                                            â”‚
â”‚  OUTPUT (to Day 3)                                        â”‚
â”‚    â†“                                                       â”‚
â”‚  Top-3 Relevant Chunks with Similarity Scores             â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5ï¸âƒ£ MAIN CONTENT (PART 1): Understanding Embeddings

### What Are Embeddings?

**Simple Explanation:**
Embeddings convert text into numbers that computers can compare. They capture *meaning* in a mathematical way.

**Example:**

```
Text: "The cat sat on the mat"
Embedding: [0.12, -0.45, 0.89, 0.02, ..., 0.33]  â† 384 numbers

Text: "A feline rested on the carpet"
Embedding: [0.11, -0.44, 0.88, 0.01, ..., 0.32]  â† Very similar numbers!
```

The embeddings are similar because the sentences mean similar things.

### Why Embeddings Enable Semantic Search

**Keyword Search:**
```
Query: "machine learning"
Document 1: "deep learning networks"  â†’ NO MATCH âŒ
Document 2: "AI and ML algorithms"     â†’ MATCH âœ… (has "ML")
```

**Semantic Search (with Embeddings):**
```
Query embedding: [0.45, -0.12, 0.78, ...]

Document 1 embedding: [0.46, -0.11, 0.79, ...]  â†’ Similarity: 0.98 âœ…
Document 2 embedding: [0.22, -0.88, 0.15, ...]  â†’ Similarity: 0.45 âœ“

Result: Both are relevant, ranked by meaning!
```

### Embedding Models: HuggingFace's `sentence-transformers`

We use **`sentence-transformers/all-MiniLM-L6-v2`**:

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

embedder = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"}  # Runs on CPU, not GPU
)

# Embed single text
embedding = embedder.embed_query("Tell me about machine learning")
print(len(embedding))  # Output: 384 (dimensions)

# Embed multiple texts
embeddings = embedder.embed_documents([
    "Machine learning is...",
    "Deep learning uses...",
    "Neural networks process..."
])
print(len(embeddings))  # Output: 3 (one per document)
```

**Why This Model?**
- âœ… **Free:** No API keys needed
- âœ… **Fast:** Runs on CPU
- âœ… **Good quality:** 384-dimensional vectors
- âœ… **Lightweight:** Model size ~90MB
- âœ… **Production-ready:** Used by thousands of projects

### Vector Dimensions Explained

```
"Machine learning" â†’ [0.12, -0.45, 0.89, ..., 0.33]  â† 384 numbers

Each dimension captures a different aspect of meaning:
- Dimension 1: Technical vs. casual language
- Dimension 2: Math concepts vs. practical applications
- Dimension 3: Depth of complexity
- ... (378 more dimensions, each meaningful)

Higher dimensions = richer representation of meaning
```

### Distance Metrics: Cosine Similarity

To find similar embeddings, we use **cosine similarity**:

```
Cosine Similarity = dot product / (norm1 Ã— norm2)
Result range: -1 to 1 (typically 0 to 1 for normalized vectors)

1.0 = Perfect similarity (same direction)
0.5 = 50% similar
0.0 = Unrelated
```

**Why Cosine Similarity?**
- Measures *direction*, not magnitude
- Fast to compute on high-dimensional vectors
- Invariant to document length
- Industry standard for embeddings

---

## 6ï¸âƒ£ QUIZ: Test Your Understanding (Part 1)

### Question Set 1: Fill in the Blanks

**Question 1:** Text embeddings convert text into ________-dimensional ________ that represent meaning.

<details>
<summary>Answer</summary>
High-dimensional (or N-dimensional) vectors
</details>

---

**Question 2:** The embedding model we use, `sentence-transformers/all-MiniLM-L6-v2`, outputs ________ dimensional vectors.

<details>
<summary>Answer</summary>
384
</details>

---

**Question 3:** ________ search finds results by meaning, while ________ search finds exact word matches.

<details>
<summary>Answer</summary>
Semantic (or similarity-based), keyword (or exact-match)
</details>

---

**Question 4:** The primary advantage of embeddings for large documents is that they enable ________ search without needing ________ indices.

<details>
<summary>Answer</summary>
Semantic (or similarity-based), keyword (or inverted)
</details>

---

**Question 5:** ________ similarity is the metric used to compare embeddings because it measures the angle between vectors, not their magnitude.

<details>
<summary>Answer</summary>
Cosine
</details>

---

## 7ï¸âƒ£ MAIN CONTENT (PART 2): FAISS & Vector Store Operations

### What is FAISS?

**FAISS (Facebook AI Similarity Search)** is a library for efficient similarity search in high-dimensional spaces.

**Why FAISS?**
- ğŸš€ **Fast:** Optimized search even with millions of vectors
- ğŸ’¾ **Memory-efficient:** Compression techniques reduce size
- ğŸ”“ **Open-source:** Free and mature
- ğŸ **Python bindings:** Easy to use with LangChain
- âš™ï¸ **Customizable:** Multiple index types for different needs

### Creating a Vector Store

```python
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Step 1: Create embeddings
embedder = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Step 2: Create vector store from documents
vector_store = FAISS.from_documents(
    documents=chunks,  # From Day 1
    embedding=embedder
)

# Result: FAISS index built and ready for search!
print(f"Stored {vector_store.index.ntotal} vectors")
```

**Under the Hood:**

```
chunks = [
    Document("Machine learning...", source="doc1.pdf"),
    Document("Deep learning...", source="doc2.pdf"),
    ...
]
      â†“
embedder.embed() for each chunk
      â†“
[0.12, -0.45, ...],  [0.11, -0.44, ...],  ...
      â†“
FAISS builds index for efficient search
      â†“
Vector Store ready!
```

### Similarity Search Operations

**Basic Similarity Search:**

```python
# Find top 3 most similar documents
results = vector_store.similarity_search(
    query="What about optimization?",
    k=3
)

for doc in results:
    print(f"Source: {doc.metadata['source']}")
    print(f"Content: {doc.page_content[:100]}...")
```

**Similarity Search with Scores:**

```python
# Get results with relevance scores
results_with_scores = vector_store.similarity_search_with_score(
    query="What about optimization?",
    k=3
)

for doc, score in results_with_scores:
    print(f"Relevance: {score:.4f}")  # 0 = perfect, 1 = no similarity
    print(f"Content: {doc.page_content[:100]}...")
```

**Batch Search (efficient for multiple queries):**

```python
queries = [
    "Machine learning",
    "Deep learning",
    "Neural networks"
]

for query in queries:
    results = vector_store.similarity_search(query, k=3)
    print(f"\nQuery: {query}")
    for doc, score in results:
        print(f"  - {doc.page_content[:50]}...")
```

### Adding and Removing Documents

**Add new documents to existing store:**

```python
# New documents arrive
new_docs = [
    Document("Reinforcement learning..."),
    Document("Transfer learning...")
]

# Add to vector store
vector_store.add_documents(new_docs)

print(f"Total documents: {vector_store.index.ntotal}")
```

**Important Note:** FAISS doesn't support deletion directly. For document management:
- Create a new index with filtered documents
- Or use alternative vector stores (Pinecone, Weaviate)

### Persistence: Save and Load

**Save vector store to disk:**

```python
# Save after building
vector_store.save_local("./faiss_index")

# Creates:
# â”œâ”€â”€ index.faiss      â† The FAISS index (binary)
# â””â”€â”€ index.pkl        â† Metadata and chunk info
```

**Load vector store later:**

```python
from langchain_community.vectorstores import FAISS

loaded_store = FAISS.load_local(
    folder_path="./faiss_index",
    embeddings=embedder
)

# Continue searching immediately!
results = loaded_store.similarity_search("query", k=3)
```

**Production Pattern:**

```python
# During setup (once)
vs = FAISS.from_documents(chunks, embedder)
vs.save_local("./faiss_index")

# During runtime (many times)
vs = FAISS.load_local("./faiss_index", embedder)
results = vs.similarity_search(user_query, k=3)
```

### Retriever Interface (for Day 3)

FAISS vector stores act as LangChain retrievers:

```python
# Convert to retriever
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

# Use in chains (Day 3)
relevant_docs = retriever.get_relevant_documents("query")
```

---

## 8ï¸âƒ£ ACTIVITY: True/False Assessment

**Directions:** Answer True or False for each statement.

**Statement 1:** Embeddings are only useful for very large documents (1000+ words).
- [ ] True
- [x] **False** â† Correct! Embeddings work great for any text, even single sentences.

---

**Statement 2:** FAISS can search through millions of vectors faster than a simple list.
- [x] **True** â† Correct! FAISS uses optimized indexing algorithms.
- [ ] False

---

**Statement 3:** You must retrain embeddings every time you add new documents.
- [ ] True
- [x] **False** â† Correct! Embedding model is fixed; new docs just get embedded and added to index.

---

**Statement 4:** Cosine similarity ranges from 0 (unrelated) to 1 (identical).
- [x] **True** â† Correct! It measures angle between vectors (direction-based).
- [ ] False

---

**Statement 5:** Saving a FAISS vector store creates just one binary file.
- [ ] True
- [x] **False** â† Correct! It creates at least two files: index.faiss and index.pkl.

---

## 9ï¸âƒ£ EXPLORE FURTHER: Deep Dive Resources

### Advanced Questions for Your Research

1. **Other Embedding Models:** How do other models (`all-mpnet-base-v2`, `BGE-small`) compare? When would you choose them?

2. **Index Types:** FAISS supports different index types (Flat, HNSW, IVF). When would you use each?

3. **Dimensionality Reduction:** How would you reduce 384 dimensions to 96? What's the trade-off?

4. **Approximate Search:** What's the difference between exact and approximate nearest neighbor search?

5. **Scaling:** How would you handle 100 million documents? What are the solutions?

6. **GPU Acceleration:** How would you accelerate embedding/search with GPU?

### Official Resources

- **FAISS Documentation:** https://github.com/facebookresearch/faiss/wiki
- **Sentence Transformers:** https://www.sbert.net/
- **LangChain FAISS Integration:** https://python.langchain.com/docs/integrations/vectorstores/faiss
- **HuggingFace Model Hub:** https://huggingface.co/models?library=sentence-transformers

### Research Papers

- "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks" (Reimers & Gurevych, 2019)
- "Billion-scale similarity search with GPUs" (Johnson et al., 2017) - FAISS paper
- "ANCE: Approximate Nearest Neighbor Negative Contrastive Learning" (Xiong et al., 2021)

---

## ğŸ”Ÿ SUMMARY: What We Learned Today

### Key Takeaways

**Embeddings Magic:**
- Text gets converted to vectors that capture meaning
- Similar texts have similar (close) vectors
- No training neededâ€”pre-trained models work great

**FAISS Power:**
- Fast similarity search in high-dimensional space
- Persistent storage for production use
- Scales to millions of documents

**Vector Store Workflow:**
1. Create embeddings from document chunks
2. Build FAISS index
3. Search by similarity, get top-K results
4. Save/load for persistence

**Why It Matters:**
- Foundation of retrieval in RAG
- Makes search semantic, not keyword-based
- Enables Day 3 (LLM generation from retrieved docs)

### Common Mistakes to Avoid

âŒ **Mistake 1:** Different embedding models for different documents  
âœ… **Fix:** Use same model for all docs and queries

âŒ **Mistake 2:** Assuming all embeddings are equal  
âœ… **Fix:** Choose models based on use case (general vs. domain-specific)

âŒ **Mistake 3:** Not saving/loading the index  
âœ… **Fix:** Always persist for production

âŒ **Mistake 4:** Ignoring similarity scores  
âœ… **Fix:** Use scores to filter low-quality results

---

## 1ï¸âƒ£1ï¸âƒ£ ENHANCE YOUR KNOWLEDGE: Additional Learning Resources

### Official Documentation & Blogs

1. **LangChain Vector Store Documentation**
   - Overview: https://python.langchain.com/docs/modules/data_connection/vectorstores/
   - FAISS Integration: https://python.langchain.com/docs/integrations/vectorstores/faiss

2. **Sentence Transformers**
   - Documentation: https://www.sbert.net/
   - Model Hub: https://huggingface.co/sentence-transformers
   - Training and Fine-tuning: https://www.sbert.net/docs/training/overview.html

3. **LangChain Blog**
   - "Embeddings: What Every Developer Should Know": https://blog.langchain.dev/
   - Vector Store Comparisons and Best Practices

4. **HuggingFace Hub**
   - Model Cards: https://huggingface.co/sentence-transformers
   - Benchmarks: https://huggingface.co/spaces/mteb/leaderboard

### Community Resources

- LangChain Discord: https://discord.gg/langchain (vector-store discussion channel)
- Reddit: r/LanguageModels (search "embeddings", "FAISS")
- GitHub: Real examples in LangChain repo

### Videos to Watch

- "Understanding Embeddings" - LangChain YouTube
- "FAISS: Advanced Similarity Search" - Facebook Research
- "Vector Databases Explained" - Various creators

---

## 1ï¸âƒ£2ï¸âƒ£ TRY IT YOURSELF: Tasks & Challenges

### Task 1: Build Your Vector Store

**Objective:** Create a complete vector store from scratch.

**Steps:**
1. Load your Day 1 documents
2. Initialize HuggingFace embeddings
3. Create FAISS vector store
4. Print total vectors stored
5. Test with 5 sample queries
6. Save the index to disk
7. Load and verify it works

**Expected Output:**
```
Loaded 3 documents
Created 45 chunks
Building embeddings... (may take ~30 seconds on first run)
Created FAISS index with 45 vectors
Saved to: ./faiss_index

Testing queries:
Query 1: "machine learning"
  âœ“ Result 1: Similarity 0.92
  âœ“ Result 2: Similarity 0.87
  âœ“ Result 3: Similarity 0.81
```

---

### Task 2: Similarity Search Analysis

**Objective:** Understand how similarity scores vary.

**Steps:**
1. Take your vector store
2. Run these queries:
   - Exact phrase from document: "machine learning algorithms"
   - Similar concept: "ML models"
   - Vague concept: "computing"
   - Unrelated term: "pizza recipes"
3. For each query, get top-3 results with scores
4. Analyze the scores:
   - Range?
   - Variance?
   - Correlation with relevance?

**Expected Insight:**
```
Query: "machine learning algorithms"
â”œâ”€ Result 1 (exact match): 0.95
â”œâ”€ Result 2 (similar): 0.88
â””â”€ Result 3 (related): 0.82

Query: "pizza recipes"
â”œâ”€ Result 1 (random): 0.25
â”œâ”€ Result 2 (random): 0.20
â””â”€ Result 3 (random): 0.18
```

---

### Task 3: Comparing Embedding Models

**Objective:** See how different models produce different results.

**Challenge:**
1. Create vector stores with 2 different embedding models:
   - `sentence-transformers/all-MiniLM-L6-v2` (384 dims, fast)
   - `sentence-transformers/all-mpnet-base-v2` (768 dims, better quality)
2. Run same queries on both
3. Compare results:
   - Are top results the same?
   - Do scores differ?
   - Which is faster?
4. Write pros/cons for each

**Expected Comparison:**
```
MiniLM (384 dims, fast):
â”œâ”€ Speed: 100 queries/sec
â”œâ”€ Top results: ["doc A", "doc B", "doc C"]
â””â”€ Avg similarity: 0.85

MPNet (768 dims, slower):
â”œâ”€ Speed: 20 queries/sec
â”œâ”€ Top results: ["doc A", "doc C", "doc B"]  â† Different ranking
â””â”€ Avg similarity: 0.88  â† Slightly higher scores
```

---

### Task 4: Production Ready Vector Store

**Objective:** Build a reusable, production-ready system.

**Challenge:**
```python
# Create a VectorStoreManager class that:
# 1. Initializes with configurable embedding model
# 2. Creates new index from documents
# 3. Adds documents incrementally
# 4. Performs search with filtering
# 5. Saves/loads from disk
# 6. Reports statistics

class VectorStoreManager:
    def __init__(self, embedding_model="default"):
        # Initialize with embedding model
        pass
    
    def create_from_documents(self, documents):
        # Build index
        pass
    
    def add_documents(self, new_documents):
        # Add to existing index
        pass
    
    def search(self, query, k=3, score_threshold=0.5):
        # Search with optional filtering
        pass
    
    def save(self, path):
        # Persist to disk
        pass
    
    def load(self, path):
        # Load from disk
        pass
    
    def stats(self):
        # Return: total vectors, disk size, etc.
        pass

# Usage:
manager = VectorStoreManager()
manager.create_from_documents(chunks)
results = manager.search("query", k=5, score_threshold=0.7)
manager.save("./production_index")
```

---

### Task 5: Query Optimization

**Objective:** Optimize search for production.

**Challenge:**
1. Time your vector store search:
   - Single query: How long?
   - 100 sequential queries: Total time?
   - Batch vs. sequential?
2. Experiment with different K values:
   - k=1, k=3, k=10, k=50
   - How do scores change?
   - How does speed change?
3. Profile embedding time:
   - How long to embed a query?
   - How long to embed 1000 documents?
4. Write recommendations for production:
   - Best k value?
   - Batch size for adding documents?
   - Update frequency?

---

### Community Discussion

**Post your answers to these on the discussion forum:**

1. What embedding model would you use for your domain (legal, medical, general)? Why?
2. Did you notice accuracy differences between MiniLM and MPNet?
3. How would you handle documents that are not in English?
4. If you had 100M documents, how would you handle search speed?

---

## ğŸ End of Day 2

### You Now Know:

âœ… How embeddings transform text to vectors  
âœ… How FAISS enables efficient search  
âœ… How to create and persist vector stores  
âœ… How to perform semantic similarity search  
âœ… How embeddings connect Day 1 to Day 3  

### Tomorrow (Day 3):

ğŸš€ We'll use these vectors to **generate answers**  
ğŸš€ We'll integrate the **Groq LLM**  
ğŸš€ We'll build the complete **RAG pipeline**  
ğŸš€ We'll add **web search** capabilities  

**Action Items Before Day 3:**
- âœ… Complete all 5 tasks above
- âœ… Build your vector store and save it
- âœ… Test queries and understand similarity scores
- âœ… Review what "retrieval" means (we'll use it tomorrow)

---

## ğŸ“š Quick Reference

### Code Snippets You'll Use

```python
# Create embeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Build vector store
from langchain_community.vectorstores import FAISS
vs = FAISS.from_documents(chunks, embedder)

# Search
results = vs.similarity_search("query", k=3)
results_with_scores = vs.similarity_search_with_score("query", k=3)

# Persist
vs.save_local("./faiss_index")
vs = FAISS.load_local("./faiss_index", embedder)
```

---

**Happy Learning! ğŸ“**

*Next: Day 3 - RAG Pipeline & Web Search*
