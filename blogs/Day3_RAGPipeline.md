# ğŸ§  Day 3: RAG Pipeline & Web Search - Creating Intelligent Responses

**Duration:** 2 hours | **Type:** Advanced Integration | **Difficulty:** Intermediate-Advanced

---

## 1ï¸âƒ£ SESSION FLOW

### What We'll Cover Today (Step-by-Step)

1. **RAG Architecture Review** (10 min)
   - How retrieval + generation works together
   - The flow: Query â†’ Retrieve â†’ Generate
   - Why this is more powerful than LLMs alone

2. **LLM Integration with Groq** (15 min)
   - Introduction to Groq API
   - Why Groq (speed, free tier, quality)
   - Setting up ChatGroq

3. **Building Prompt Templates** (15 min)
   - Context injection in prompts
   - Source attribution strategies
   - Testing prompts

4. **LangChain Chains & LCEL** (20 min)
   - What are chains?
   - LCEL (LangChain Expression Language)
   - Building your first RAG chain

5. **Implementing Web Search** (15 min)
   - Tavily API overview
   - Hybrid search (documents + web)
   - When to use each source

6. **Streaming Responses** (15 min)
   - Real-time response generation
   - Streaming in action
   - User experience benefits

7. **Integration & Testing** (10 min)
   - Full end-to-end RAG pipeline
   - Testing and debugging
   - Q&A

---

## 2ï¸âƒ£ LEARNING OBJECTIVES

By the end of Day 3, students will be able to:

âœ… **Understand RAG Orchestration**
- Explain the complete RAG workflow
- Understand when to retrieve vs. generate
- Design prompts for context injection
- Track and cite sources

âœ… **Master LLM Integration**
- Use Groq API for fast, free LLM access
- Control temperature and max tokens
- Handle streaming responses
- Implement error handling

âœ… **Build LangChain Chains**
- Write LCEL expressions
- Chain retrievers to LLMs
- Add custom processing steps
- Debug chain execution

âœ… **Enhance with Web Search**
- Integrate Tavily web search
- Combine document + web results
- Implement hybrid search logic
- Format results for LLMs

âœ… **Apply SOLID Principles**
- Each component has single responsibility
- Easy to replace components (plug-and-play)
- Chains are composable and testable

**Prerequisites:** Day 1-2 knowledge, understanding of prompts and LLMs

**Key Concepts:** RAG pipeline, chains, LCEL, streaming, web search, source tracking, prompt engineering

---

## 3ï¸âƒ£ THEME: THE KNOWLEDGE SYNTHESIZER

### Real-World Context

**Scenario:** You're building customer support for a tech company.

**Before RAG (Traditional LLM):**
- Customer: "How do I set up OAuth in your API?"
- LLM: "OAuth is an authentication protocol. You would typically..."
- Problem: Generic answer, not your API's specifics âŒ

**With RAG (Today's Solution):**
- Customer: "How do I set up OAuth in your API?"
- System: Retrieves your OAuth documentation
- System: Passes it to LLM with context
- LLM: "In your API, OAuth is configured by..."
- Result: Specific, accurate, sourced answer âœ…

**Why This Day?**
- Days 1-2 prepared documents (loaded â†’ embedded)
- Day 3 uses those documents to generate answers (RAG in action)
- Day 4 builds the user interface
- Today is where everything comes together

---

## 4ï¸âƒ£ PRIMARY GOAL

### What You'll Build

A **Complete RAG System** that:

1. âœ… Retrieves relevant documents by semantic search
2. âœ… Passes context to an LLM
3. âœ… Generates accurate, sourced answers
4. âœ… Streams responses in real-time
5. âœ… Optionally searches the web for current information
6. âœ… Tracks and cites sources

### Architecture: Day 3 in the RAG Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              COMPLETE RAG PIPELINE                        â”‚
â”‚                                                            â”‚
â”‚  USER QUERY                                               â”‚
â”‚    "Tell me about machine learning"                       â”‚
â”‚    â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ RETRIEVAL (Using Day 2 Vector Store)                â”‚ â”‚
â”‚  â”‚ â”œâ”€ Convert query to embedding                       â”‚ â”‚
â”‚  â”‚ â”œâ”€ Search FAISS for similar chunks                  â”‚ â”‚
â”‚  â”‚ â””â”€ Retrieve top-3 relevant documents                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚    â†“                                                       â”‚
â”‚  CONTEXT DOCUMENTS                                         â”‚
â”‚    "Machine learning is supervised and unsupervised...   â”‚
â”‚     Supervised learning uses labeled data..."             â”‚
â”‚    â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ AUGMENTED PROMPT                                     â”‚ â”‚
â”‚  â”‚ System: You are helpful. Use context to answer.     â”‚ â”‚
â”‚  â”‚ Context: [Retrieved documents]                      â”‚ â”‚
â”‚  â”‚ Question: Tell me about machine learning             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚    â†“                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ LLM (Groq - Fast, Free)                             â”‚ â”‚
â”‚  â”‚ â”œâ”€ Process prompt + context                         â”‚ â”‚
â”‚  â”‚ â”œâ”€ Generate answer                                  â”‚ â”‚
â”‚  â”‚ â””â”€ Stream response tokens                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚    â†“                                                       â”‚
â”‚  GENERATED ANSWER                                          â”‚
â”‚    "Machine learning includes supervised and...           â”‚
â”‚     Source: ml_guide.pdf (page 2), ml_intro.txt"          â”‚
â”‚                                                            â”‚
â”‚  OPTIONAL: WEB SEARCH                                      â”‚
â”‚    If answer incomplete â†’ Search the web                  â”‚
â”‚    Combine document + web results                         â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5ï¸âƒ£ MAIN CONTENT (PART 1): RAG Orchestration & LLM Integration

### Understanding the RAG Flow

**Step-by-step:**

```
1. QUERY ARRIVES
   â†“
2. RETRIEVE PHASE
   â”œâ”€ Find relevant documents (from vector store)
   â”œâ”€ Rank by similarity
   â””â”€ Select top-K
   â†“
3. AUGMENTATION PHASE
   â”œâ”€ Format retrieved documents
   â”œâ”€ Inject into prompt
   â””â”€ Create "augmented prompt"
   â†“
4. GENERATION PHASE
   â”œâ”€ Send augmented prompt to LLM
   â”œâ”€ LLM reads context
   â”œâ”€ LLM generates answer
   â””â”€ Stream response
   â†“
5. POST-PROCESSING
   â”œâ”€ Attach source information
   â”œâ”€ Format for user
   â””â”€ Return to user
```

### Introducing Groq: Fast, Free LLM API

**Why Groq?**

| Feature | Groq | OpenAI | Claude |
|---------|------|--------|--------|
| **Cost** | FREE (14k+ req/min) | $0.015/1K tokens | $0.80/1M tokens |
| **Speed** | ~500 tokens/sec | ~200 tokens/sec | ~150 tokens/sec |
| **Model** | Llama 3.1 8B-70B | GPT-4 | Claude 3 |
| **Setup** | 1 API key | 1 API key + billing | 1 API key + billing |
| **Perfect for** | Teaching, prototypes | Production | Enterprise |

**Getting Started:**

```python
from langchain_groq import ChatGroq

llm = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0.7,           # Creativity (0=deterministic, 1=creative)
    max_tokens=1000,           # Max response length
    groq_api_key="gsk_..."     # From console.groq.com
)

# Use it like any LangChain LLM
response = llm.invoke("Tell me about Python")
print(response.content)
```

**Model Options:**

```python
# Fast and lightweight (perfect for our use case)
"llama-3.1-8b-instant"

# More powerful but slower
"llama-3.1-70b-versatile"

# Better at reasoning
"mixtral-8x7b-32768"
```

### Prompt Templates for RAG

**Basic Structure:**

```python
from langchain.prompts import PromptTemplate

template = """You are a helpful assistant. Use the provided context to answer the question.
If you don't know the answer, say "I don't know" rather than making something up.

Context:
{context}

Question: {question}

Answer:"""

prompt = PromptTemplate.from_template(template)

# Use it
formatted_prompt = prompt.format(
    context="Machine learning is...",
    question="What is ML?"
)
```

**Advanced: RAG Prompt with Source Tracking**

```python
from langchain.prompts import PromptTemplate

rag_template = """You are a helpful assistant that answers questions based on documents.

{context}

Question: {question}

Provide your answer and cite the source document(s) that support it."""

rag_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=rag_template
)
```

**How Context Gets Injected:**

```
Template: "Answer: {context}"
         
Documents retrieved:
  - "ML uses algorithms to learn patterns"
  - "Supervised learning needs labels"

Context becomes:
  Document 1: "ML uses algorithms to learn patterns"
  Document 2: "Supervised learning needs labels"

Final prompt:
  "Answer: 
   Document 1: ML uses algorithms to learn patterns
   Document 2: Supervised learning needs labels"
```

### Building Basic Chains

**LangChain Chains** connect components together. The simplest version:

```python
from langchain.chains import LLMChain

# Create a chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use it
response = chain.run(question="What is ML?", context="...")
print(response)
```

---

## 6ï¸âƒ£ QUIZ: Test Your Understanding (Part 1)

### Question Set 1: Fill in the Blanks

**Question 1:** RAG orchestration follows three phases: ________, ________, and ________.

<details>
<summary>Answer</summary>
Retrieval, Augmentation, Generation (or: Retrieve, Augment, Generate)
</details>

---

**Question 2:** Groq is particularly useful for teaching because it offers ________ tier API access with ________ response times.

<details>
<summary>Answer</summary>
Free (or generous), fast (or instant)
</details>

---

**Question 3:** A ________ injects retrieved documents into a prompt template to provide context to the LLM.

<details>
<summary>Answer</summary>
PromptTemplate (or: prompt template)
</details>

---

**Question 4:** ________ determines how "creative" an LLM's responses are (0=deterministic, 1=highly creative).

<details>
<summary>Answer</summary>
Temperature
</details>

---

**Question 5:** The main advantage of RAG over a fine-tuned LLM is that you can update information without ________.

<details>
<summary>Answer</summary>
Retraining (or: fine-tuning again)
</details>

---

## 7ï¸âƒ£ MAIN CONTENT (PART 2): LCEL Chains & Web Search

### Introduction to LCEL (LangChain Expression Language)

LCEL lets you compose chains with elegant, readable syntax:

```python
from langchain_core.runnables import RunnablePassthrough

# Traditional way (complex)
chain = LLMChain(llm=llm, prompt=prompt)

# LCEL way (readable)
chain = prompt | llm

# LCEL with retriever (RAG)
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)
```

**Breaking it down:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: Query   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    {"context": retriever, "question": RunnablePassthrough()}
         â”‚
    â”œâ”€ Get context: retriever(query)
    â””â”€ Pass query through: query
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ prompt(context, q)   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ llm(prompt)       â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Output: Answer    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Complete RAG Chain Example

```python
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. Set up retriever (from Day 2)
vector_store = FAISS.load_local("./faiss_index", embedder)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

# 2. Create LLM
llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0.7)

# 3. Create prompt
template = """Answer based on context:
{context}
Question: {question}"""
prompt = PromptTemplate.from_template(template)

# 4. Build chain with LCEL
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 5. Use it
answer = chain.invoke("What is machine learning?")
print(answer)
```

### Streaming Responses

**Without Streaming (traditional):**
```
chain.invoke(query)
â† [waits 10 seconds]
â† Returns complete answer
```

**With Streaming (modern UX):**
```
chain.stream(query)
â† Returns "Machine"
â† Returns "learning"
â† Returns "is"
â† ... [user sees response building in real-time]
```

**Implementation:**

```python
# Streaming with LCEL
for chunk in chain.stream({"question": "What is ML?"}):
    print(chunk, end="", flush=True)  # Print as it arrives
```

### Implementing Web Search with Tavily

**Why Web Search?**

- Documents might be outdated
- Users ask about current events
- Hybrid approach: Documents + Web = Best of both

**Setting up Tavily:**

```python
from tools.tavily_search import TavilySearchTool

search = TavilySearchTool(api_key="tvly_...")

# Search the web
results = search.search(
    query="latest machine learning trends 2024",
    max_results=3
)

for result in results:
    print(f"Title: {result['title']}")
    print(f"Link: {result['link']}")
    print(f"Content: {result['content']}\n")
```

**Combining Document + Web Search:**

```python
# Get document results
doc_results = retriever.invoke(query)

# Get web results
web_results = search.search(query, max_results=2)

# Combine and format for LLM
combined_context = f"""
Document sources:
{format_docs(doc_results)}

Recent web sources:
{format_web_results(web_results)}
"""

# Pass to RAG
answer = rag_chain.invoke({
    "context": combined_context,
    "question": query
})
```

### Source Tracking and Attribution

**Why Important?**
- Users trust sources
- Legal requirements in some domains
- Reproducible answers

**Implementation:**

```python
# Retrieve with sources
results_with_scores = vector_store.similarity_search_with_score(
    query="machine learning",
    k=3
)

# Track sources
sources = []
context_text = ""

for doc, score in results_with_scores:
    source = doc.metadata.get("source", "Unknown")
    sources.append(source)
    context_text += f"[{source}]: {doc.page_content}\n"

# Generate answer
answer = llm.invoke(prompt.format(context=context_text, question=query))

# Attach sources
output = {
    "answer": answer.content,
    "sources": list(set(sources)),  # Unique sources
    "confidence": score  # Optional relevance score
}
```

---

## 8ï¸âƒ£ ACTIVITY: True/False Assessment

**Directions:** Answer True or False for each statement.

**Statement 1:** RAG requires the LLM to have access to internet during generation.
- [ ] True
- [x] **False** â† Correct! RAG provides documents upfront; no internet access needed.

---

**Statement 2:** Using Groq instead of OpenAI means lower quality answers.
- [ ] True
- [x] **False** â† Correct! Groq's Llama 3.1 is high quality and often faster.

---

**Statement 3:** Temperature should be 0 for all use cases.
- [ ] True
- [x] **False** â† Correct! Temperature depends on task (0 for facts, 0.8 for creativity).

---

**Statement 4:** Web search in RAG is always necessary.
- [ ] True
- [x] **False** â† Correct! Optional; use only for questions that need current info.

---

**Statement 5:** LCEL chains are purely syntactic sugar with no functional benefit.
- [ ] True
- [x] **False** â† Correct! LCEL enables streaming, parallelization, and better optimization.

---

## 9ï¸âƒ£ EXPLORE FURTHER: Deep Dive Resources

### Advanced Questions for Your Research

1. **Alternative LLMs:** How would you integrate Claude, GPT-4, or Mixtral instead of Groq? What changes?

2. **Prompt Engineering:** How would you optimize prompts for different question types (factual, reasoning, creative)?

3. **Chain Debugging:** What tools can you use to debug LCEL chain execution? How would you trace errors?

4. **Hybrid Search:** Beyond document + web, what other sources could you integrate (APIs, databases)?

5. **Response Quality:** How would you measure RAG answer quality? What metrics matter?

6. **Caching:** How could you cache LLM responses to reduce API calls?

### Official Resources

- **LangChain Chains Documentation:** https://python.langchain.com/docs/modules/chains/
- **LCEL Documentation:** https://python.langchain.com/docs/expression_language/
- **Groq Documentation:** https://console.groq.com/docs
- **Tavily Search API:** https://tavily.com/
- **LangChain + Groq Integration:** https://python.langchain.com/docs/integrations/llms/groq

### Research Papers

- "In-Context Learning with Long-Context Models" (various authors)
- "Retrieval-Augmented Generation for Knowledge-Intensive Tasks" (Lewis et al., 2020)
- "LangChain: Building Production-Ready LLM Applications" (technical blog posts)

---

## ğŸ”Ÿ SUMMARY: What We Learned Today

### Key Takeaways

**RAG Pipeline:**
- Retrieve relevant documents
- Augment prompt with context
- Generate answers grounded in facts

**LLM Integration:**
- Groq provides fast, free API access
- Temperature controls creativity
- Streaming improves UX

**LCEL Chains:**
- Elegant, readable syntax
- Composable components
- Enables optimization and streaming

**Web Search:**
- Optional enhancement
- Combines documents + internet
- Improves answer currency

**Source Tracking:**
- Essential for credibility
- Enables verification
- Required in many domains

### Common Mistakes to Avoid

âŒ **Mistake 1:** Retrieving too many/few documents  
âœ… **Fix:** Start with k=3, tune based on results

âŒ **Mistake 2:** Not controlling temperature  
âœ… **Fix:** Use 0.3 for facts, 0.8 for creative tasks

âŒ **Mistake 3:** Ignoring sources in prompt  
âœ… **Fix:** Always include source attribution in template

âŒ **Mistake 4:** Using web search for everything  
âœ… **Fix:** Use hybrid only when freshness matters

---

## 1ï¸âƒ£1ï¸âƒ£ ENHANCE YOUR KNOWLEDGE: Additional Learning Resources

### Official Documentation & Blogs

1. **LangChain Expression Language (LCEL)**
   - Tutorial: https://python.langchain.com/docs/expression_language/
   - Advanced patterns: https://python.langchain.com/docs/expression_language/composition

2. **Groq API**
   - Console: https://console.groq.com/
   - Documentation: https://console.groq.com/docs/
   - Models: https://console.groq.com/docs/models

3. **LangChain Chains**
   - Overview: https://python.langchain.com/docs/modules/chains/
   - LCEL Cookbook: https://python.langchain.com/docs/expression_language/cookbook/

4. **Tavily Search Integration**
   - API Docs: https://tavily.com/
   - LangChain Integration: https://python.langchain.com/docs/integrations/tools/tavily

### Community Resources

- LangChain Discord: https://discord.gg/langchain
- GitHub Discussions: https://github.com/langchain-ai/langchain/discussions
- Reddit: r/LanguageModels, r/OpenAI

### Videos to Watch

- "Building RAG Systems with LangChain" - LangChain YouTube
- "LCEL Tutorial" - Official LangChain
- "Prompt Engineering for RAG" - DeepLearning.AI

---

## 1ï¸âƒ£2ï¸âƒ£ TRY IT YOURSELF: Tasks & Challenges

### Task 1: Build Your RAG Chain

**Objective:** Create a complete end-to-end RAG system.

**Steps:**
1. Load your Day 2 vector store
2. Initialize Groq LLM
3. Create a RAG prompt template
4. Build the LCEL chain
5. Test with 5 different queries
6. Save the results

**Expected Output:**
```
Query: "What is machine learning?"

Retrieved context:
â”œâ”€ Document 1 (score 0.92): "ML is a subset of AI..."
â”œâ”€ Document 2 (score 0.87): "Supervised learning uses..."
â””â”€ Document 3 (score 0.81): "Neural networks..."

Generated Answer:
Machine learning is a subset of artificial intelligence
that enables systems to learn and improve... [sources: doc1.pdf, doc2.txt]
```

---

### Task 2: Temperature & Response Variation

**Objective:** Understand how temperature affects responses.

**Steps:**
1. Take one query
2. Generate answers with temperatures: 0.0, 0.3, 0.7, 1.0
3. Compare responses:
   - Are they consistent?
   - How creative do they get?
   - Which is best for your use case?
4. Document observations

**Expected Pattern:**
```
Temperature 0.0 (Deterministic):
â”œâ”€ Response 1: "Machine learning is the field..."
â”œâ”€ Response 2: "Machine learning is the field..."  â† Identical
â””â”€ Response 3: "Machine learning is the field..."

Temperature 1.0 (Creative):
â”œâ”€ Response 1: "ML lets computers groove to data..."
â”œâ”€ Response 2: "Think of ML as teaching..."
â””â”€ Response 3: "Algorithms absorb patterns..."  â† All different!
```

---

### Task 3: Hybrid Search (Documents + Web)

**Objective:** Implement hybrid search combining documents and web.

**Challenge:**
1. Create a query that would benefit from current information
   (e.g., "Latest AI trends 2024")
2. Implement hybrid search:
   - Get document results
   - Get web results
   - Combine into single context
3. Generate answer with combined context
4. Compare with document-only approach
5. Which is better? When?

**Expected Comparison:**
```
Document-Only:
- Sources: training_materials.pdf (2022)
- Accuracy: Medium
- Freshness: Low

Hybrid (Document + Web):
- Sources: training_materials.pdf + 5 recent articles
- Accuracy: High
- Freshness: High
```

---

### Task 4: Source Tracking System

**Objective:** Build a system that tracks and attributes sources.

**Challenge:**
```python
# Create a SourceTrackingRAG class that:
# 1. Retrieves documents with sources
# 2. Generates answer
# 3. Maps which source contributed to which answer part
# 4. Outputs answer + detailed attribution

class SourceTrackingRAG:
    def __init__(self, chain, retriever):
        # Initialize
        pass
    
    def query_with_sources(self, question):
        # Get documents with metadata
        pass
    
    def get_attribution(self):
        # Return: answer + source mapping
        pass

# Usage:
rag = SourceTrackingRAG(chain, retriever)
result = rag.query_with_sources("What is ML?")
print(f"Answer: {result['answer']}")
print(f"Sources: {result['sources']}")
print(f"Confidence: {result['confidence']}")
```

---

### Task 5: Chain Optimization & Debugging

**Objective:** Optimize RAG chain performance.

**Challenge:**
1. Build your RAG chain
2. Time the execution:
   - Retrieval time
   - LLM generation time
   - Total time
3. Experiment with optimizations:
   - Different k values (1, 3, 5, 10)
   - Different model sizes
   - Caching strategies
4. Profile memory usage:
   - Loaded vector store size
   - LLM memory footprint
   - Total memory
5. Write optimization recommendations

**Expected Output:**
```
Baseline:
â”œâ”€ Retrieval: 0.2s
â”œâ”€ LLM: 5.3s
â””â”€ Total: 5.5s

Optimized (k=3 instead of 10):
â”œâ”€ Retrieval: 0.1s
â”œâ”€ LLM: 5.2s
â””â”€ Total: 5.3s  â† 3.6% faster
```

---

### Community Discussion

**Post your answers to these on the discussion forum:**

1. What temperature did you find optimal for your use case?
2. Did hybrid search improve your results? How?
3. What was the most challenging part of debugging your chain?
4. How would you handle very long context (100+ documents)?

---

## ğŸ End of Day 3

### You Now Know:

âœ… The complete RAG pipeline (retrieve â†’ augment â†’ generate)  
âœ… How to integrate Groq LLM  
âœ… How to build LCEL chains  
âœ… How to control response quality  
âœ… How to track and cite sources  
âœ… How to integrate web search  

### Tomorrow (Day 4):

ğŸš€ We'll build a **Streamlit UI**  
ğŸš€ We'll create **file upload** functionality  
ğŸš€ We'll implement **chat history**  
ğŸš€ We'll deploy the **complete application**  

**Action Items Before Day 4:**
- âœ… Complete all 5 tasks above
- âœ… Build and test your complete RAG chain
- âœ… Experiment with different prompts and temperatures
- âœ… Understand the full end-to-end flow

---

## ğŸ“š Quick Reference

### Code Snippets You'll Use

```python
# Setup LLM
from langchain_groq import ChatGroq
llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0.7)

# Create retriever
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

# Build LCEL chain
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Use it
answer = chain.invoke("your question")

# Stream it
for chunk in chain.stream({"question": "your question"}):
    print(chunk, end="", flush=True)
```

---

**Happy Learning! ğŸ“**

*Next: Day 4 - Streamlit UI & Deployment*
